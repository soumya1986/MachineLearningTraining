{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "angry-problem",
   "metadata": {},
   "source": [
    "### Classification Accuracy and its Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-backing",
   "metadata": {},
   "source": [
    "Classification accuracy is the ratio of correct predictions to total predictions made.  \n",
    "**classification accuracy = correct predictions / total predictions**  \n",
    "It is often presented as a percentage by multiplying the result by 100.\n",
    "**classification accuracy = correct predictions / total predictions * 100**   \n",
    "  \n",
    "Classification accuracy can also easily be turned into a misclassification rate or error rate by inverting the value, such as:  \n",
    "**error rate = (1 - (correct predictions / total predictions)) * 100**\n",
    "\n",
    "Classification accuracy is a great place to start, but often encounters problems in practice.  \n",
    "  \n",
    "The main problem with classification accuracy is that it hides the detail you need to better understand the performance of your classification model. There are two examples where you are most likely to encounter this problem:  \n",
    "  \n",
    "- When your data has more than 2 classes. With 3 or more classes you may get a classification accuracy of 80%, but you donâ€™t know if that is because all classes are being predicted equally well or whether one or two classes are being neglected by the model.\n",
    "- When your data does not have an even number of classes. You may achieve accuracy of 90% or more, but this is not a good score if 90 records for every 100 belong to one class and you can achieve this score by always predicting the most common class value.  \n",
    "\n",
    "Classification accuracy can hide the detail you need to diagnose the performance of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-competition",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-annex",
   "metadata": {},
   "source": [
    "A confusion matrix is a summary of prediction results on a classification problem.  \n",
    "The number of correct and incorrect predictions are summarized with count values and broken down by each class. \n",
    "This is the key to the confusion matrix.  \n",
    "The confusion matrix shows the ways in which your classification model is confused when it makes predictions.  \n",
    "It gives you insight not only into the errors being made by your classifier but more importantly the types of errors that are being made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-scottish",
   "metadata": {},
   "source": [
    "Below is the process for calculating a confusion Matrix.  \n",
    "- You need a test dataset or a validation dataset with expected outcome values.\n",
    "- Make a prediction for each row in your test dataset.\n",
    "- From the expected outcomes and predictions count\n",
    "    - The number of correct predictions for each class.\n",
    "    - The number of incorrect predictions for each class, organized by the class that was predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-affair",
   "metadata": {},
   "source": [
    "These numbers are then organized into a table, or a matrix as follows:\n",
    "\n",
    "- Expected down the side: Each row of the matrix corresponds to a predicted class.\n",
    "- Predicted across the top: Each column of the matrix corresponds to an actual class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "thousand-durham",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-c226f4b937c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# confusion matrix in sklearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# confusion matrix in sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-guest",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
